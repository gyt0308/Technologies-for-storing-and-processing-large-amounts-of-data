#Task1

1、Получить доступ к данным из Hive, использовать разные операторы SQL (WITH, GROUP BY, ORDER BY, LIMIT, ASC/DESC, OVER), посмотреть на производительность​
2、Получить доступ к данным из Spark, проделав те же самые операции над RDD, DataFrame, DataSet. Замерить время выполнения

#Task2

Обработать датасет из ДЗ 1, используя минимум 3 различных преобразования rdd: map, flatMap, reduceByKey, ... 

#Task3

1. Собрать виртуальное окружение при помощи Conda, используя хотя бы один пакет, который не является транзитивной зависимостью Spark (к примеру, pandas - является).
2. Запустить код из ДЗ 2 (при необходимости добавьте туда импорт и использование библиотеки, добавленной на шаге 1) на кластере с помощью собранного виртуального окружения, убедившись, что библиотеки оттуда не только корректно импортируются, но и правильно работают.
